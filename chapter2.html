<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Behind the Scenes: How LLMs Work - Ask The Model</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&family=Fira+Code&display=swap" rel="stylesheet">
  <script>
    tailwind.config = {
      theme: {
        extend: {
          fontFamily: {
            'inter': ['Inter', 'sans-serif'],
            'fira': ['Fira Code', 'monospace']
          }
        }
      }
    }
  </script>
</head>
<body class="bg-gray-900 text-gray-200 font-inter min-h-screen flex flex-col">
  <!-- Header Component -->
  <ask-the-model-header></ask-the-model-header>

  <div class="flex flex-col lg:flex-row flex-1">
    <!-- Sidebar Component -->
    <ask-the-model-sidebar></ask-the-model-sidebar>

    <!-- Main Content -->
    <main class="flex-1 p-4 md:p-8 lg:p-12 max-w-4xl mx-auto">
      <!-- Chapter Header -->
      <div class="mb-8">
        <h1 class="text-4xl md:text-5xl font-bold text-purple-400 mb-4">Behind the Scenes: How LLMs Work</h1>
        <p class="text-xl text-gray-300">Peek under the hood of these fascinating AI systems</p>
      </div>

      <!-- Architecture Overview -->
      <div class="mb-8 bg-gradient-to-r from-purple-900/20 to-indigo-900/20 p-6 rounded-lg border border-purple-500/30">
        <h2 class="text-2xl font-semibold text-purple-400 mb-6 text-center">LLM Architecture Stack</h2>
        <div class="space-y-3">
          <div class="bg-red-800/30 p-3 rounded-lg border border-red-500/30 text-center">
            <span class="text-sm font-semibold text-red-300">Output Layer (Vocabulary Prediction)</span>
          </div>
          <div class="bg-purple-800/30 p-3 rounded-lg border border-purple-500/30 text-center">
            <span class="text-sm font-semibold text-purple-300">Transformer Layer N (Self-Attention + FFN)</span>
          </div>
          <div class="text-center text-purple-400">‚ãÆ</div>
          <div class="bg-purple-800/30 p-3 rounded-lg border border-purple-500/30 text-center">
            <span class="text-sm font-semibold text-purple-300">Transformer Layer 1 (Self-Attention + FFN)</span>
          </div>
          <div class="bg-blue-800/30 p-3 rounded-lg border border-blue-500/30 text-center">
            <span class="text-sm font-semibold text-blue-300">Input Embeddings + Positional Encoding</span>
          </div>
        </div>
      </div>

      <!-- Chapter Content -->
      <div class="space-y-8">
        <section class="bg-gray-800 p-6 rounded-lg border border-gray-700">
          <div class="flex items-start space-x-4">
            <div class="flex-shrink-0">
              <div class="w-12 h-12 bg-gradient-to-br from-purple-500 to-indigo-500 rounded-lg flex items-center justify-center">
                <span class="text-2xl">üèóÔ∏è</span>
              </div>
            </div>
            <div class="flex-1">
              <h2 class="text-2xl font-semibold text-purple-400 mb-4">Transformer Architecture</h2>
              <p class="text-gray-300 mb-4 leading-relaxed">
                LLMs use <strong>decoder-only transformer architectures</strong> that process input and output as a single sequence, predicting each token based on all preceding tokens.
              </p>

              <div class="grid grid-cols-2 md:grid-cols-4 gap-3">
                <div class="bg-blue-800/30 p-3 rounded-lg text-center border border-blue-500/20">
                  <div class="text-lg mb-1">üî¢</div>
                  <h4 class="text-xs font-semibold text-blue-300">Embeddings</h4>
                  <p class="text-xs text-gray-400">Words ‚Üí Vectors</p>
                </div>
                <div class="bg-green-800/30 p-3 rounded-lg text-center border border-green-500/20">
                  <div class="text-lg mb-1">üìç</div>
                  <h4 class="text-xs font-semibold text-green-300">Position</h4>
                  <p class="text-xs text-gray-400">Sequence Order</p>
                </div>
                <div class="bg-purple-800/30 p-3 rounded-lg text-center border border-purple-500/20">
                  <div class="text-lg mb-1">üéØ</div>
                  <h4 class="text-xs font-semibold text-purple-300">Attention</h4>
                  <p class="text-xs text-gray-400">Focus Points</p>
                </div>
                <div class="bg-red-800/30 p-3 rounded-lg text-center border border-red-500/20">
                  <div class="text-lg mb-1">‚ö°</div>
                  <h4 class="text-xs font-semibold text-red-300">Feed-Forward</h4>
                  <p class="text-xs text-gray-400">Transform Data</p>
                </div>
              </div>
            </div>
          </div>
        </section>

        <section class="bg-gray-800 p-6 rounded-lg border border-gray-700">
          <div class="flex items-start space-x-4">
            <div class="flex-shrink-0">
              <div class="w-12 h-12 bg-gradient-to-br from-green-500 to-teal-500 rounded-lg flex items-center justify-center">
                <span class="text-2xl">üéØ</span>
              </div>
            </div>
            <div class="flex-1">
              <h2 class="text-2xl font-semibold text-purple-400 mb-4">Self-Attention Mechanism</h2>
              <p class="text-gray-300 mb-4 leading-relaxed">
                <strong>Self-attention finds connections between words in a sentence</strong>. It asks: "Which other words help me understand this word?"
              </p>

              <!-- Attention Visualization Example -->
              <div class="bg-gray-700 p-4 rounded-lg mb-4">
                <h3 class="text-sm font-semibold text-purple-300 mb-3 text-center">Attention in Action</h3>
                <div class="bg-gray-800 p-3 rounded text-center">
                  <div class="text-sm text-gray-300 mb-2">Sentence: "The cat sat on the mat"</div>
                  <div class="flex items-center justify-center space-x-2 text-xs">
                    <span class="px-2 py-1 bg-blue-600 rounded">The</span>
                    <span class="px-2 py-1 bg-purple-600 rounded font-bold">cat</span>
                    <span class="px-2 py-1 bg-green-600 rounded">sat</span>
                    <span class="px-2 py-1 bg-yellow-600 rounded">on</span>
                    <span class="px-2 py-1 bg-blue-600 rounded">the</span>
                    <span class="px-2 py-1 bg-red-600 rounded">mat</span>
                  </div>
                  <div class="text-xs text-gray-400 mt-2">
                    When processing "cat", attention focuses most on "sat" and "mat" for context
                  </div>
                </div>
              </div>

              <!-- Simplified QKV -->
              <div class="grid grid-cols-3 gap-3 mb-4">
                <div class="bg-blue-800/30 p-3 rounded-lg text-center border border-blue-500/20">
                  <div class="text-lg mb-1">‚ùì</div>
                  <h4 class="text-xs font-semibold text-blue-300">Query (Q)</h4>
                  <p class="text-xs text-gray-400">"What am I looking for?"</p>
                </div>
                <div class="bg-green-800/30 p-3 rounded-lg text-center border border-green-500/20">
                  <div class="text-lg mb-1">üîë</div>
                  <h4 class="text-xs font-semibold text-green-300">Key (K)</h4>
                  <p class="text-xs text-gray-400">"What can I offer?"</p>
                </div>
                <div class="bg-red-800/30 p-3 rounded-lg text-center border border-red-500/20">
                  <div class="text-lg mb-1">üíé</div>
                  <h4 class="text-xs font-semibold text-red-300">Value (V)</h4>
                  <p class="text-xs text-gray-400">"Here's my information"</p>
                </div>
              </div>

              <div class="bg-purple-900/30 p-3 rounded-lg text-center">
                <span class="text-xs text-gray-300">üßÆ Math: attention finds connections ‚Üí weights ‚Üí combines information</span>
              </div>
            </div>
          </div>
        </section>

        <section class="bg-gray-800 p-6 rounded-lg border border-gray-700">
          <h2 class="text-2xl font-semibold text-purple-400 mb-4">The Training Process</h2>
          <p class="text-gray-300 mb-4 leading-relaxed">
            LLM training involves <strong>three key phases</strong>:
          </p>
          <div class="grid grid-cols-1 md:grid-cols-3 gap-4 mb-4">
            <div class="bg-gray-700 p-4 rounded-lg text-center">
              <div class="text-2xl font-bold text-purple-400 mb-2">1</div>
              <h3 class="text-lg font-semibold text-purple-300 mb-2">Forward Propagation</h3>
              <p class="text-gray-300 text-sm">Input tokens flow through transformer layers, producing probability distributions over vocabulary</p>
            </div>
            <div class="bg-gray-700 p-4 rounded-lg text-center">
              <div class="text-2xl font-bold text-purple-400 mb-2">2</div>
              <h3 class="text-lg font-semibold text-purple-300 mb-2">Loss Calculation</h3>
              <p class="text-gray-300 text-sm"><strong>Cross-entropy loss</strong> measures prediction accuracy against ground truth</p>
            </div>
            <div class="bg-gray-700 p-4 rounded-lg text-center">
              <div class="text-2xl font-bold text-purple-400 mb-2">3</div>
              <h3 class="text-lg font-semibold text-purple-300 mb-2">Backpropagation</h3>
              <p class="text-gray-300 text-sm"><strong>Gradient descent</strong> updates parameters to minimize loss</p>
            </div>
          </div>
          <p class="text-gray-300 leading-relaxed">
            <strong>Backpropagation efficiently computes gradients by propagating errors backward through network layers</strong>, updating millions of parameters via <strong>chain rule differentiation</strong>. This process allows the model to learn complex patterns from data through iterative optimization.
          </p>
        </section>

        <section class="bg-gray-800 p-6 rounded-lg border border-gray-700">
          <h2 class="text-2xl font-semibold text-purple-400 mb-4">Attention Mechanisms</h2>
          <p class="text-gray-300 mb-4 leading-relaxed">
            The transformer architecture uses multiple attention mechanisms to process information effectively:
          </p>
          <div class="grid grid-cols-1 md:grid-cols-2 gap-4">
            <div class="bg-gray-700 p-4 rounded-lg">
              <h3 class="text-lg font-semibold text-purple-300 mb-2">Multi-Head Attention</h3>
              <p class="text-gray-300 text-sm">Multiple attention heads run in parallel, each focusing on different aspects of the input. This allows the model to attend to various relationships simultaneously.</p>
            </div>
            <div class="bg-gray-700 p-4 rounded-lg">
              <h3 class="text-lg font-semibold text-purple-300 mb-2">Scaled Dot-Product</h3>
              <p class="text-gray-300 text-sm">Attention scores are computed using scaled dot-product, where the scaling factor prevents gradients from becoming too large during training.</p>
            </div>
          </div>
        </section>

        <section class="bg-gray-800 p-6 rounded-lg border border-gray-700">
          <h2 class="text-2xl font-semibold text-purple-400 mb-4">Layer Normalization</h2>
          <p class="text-gray-300 mb-4 leading-relaxed">
            <strong>Layer normalization</strong> is crucial for training stability in deep transformer networks. It normalizes the inputs to each layer, helping to:
          </p>
          <div class="bg-gray-700 p-4 rounded-lg">
            <ul class="text-gray-300 text-sm space-y-2">
              <li>‚Ä¢ <strong>Stabilize training:</strong> Prevent gradient explosion or vanishing</li>
              <li>‚Ä¢ <strong>Accelerate convergence:</strong> Allow higher learning rates</li>
              <li>‚Ä¢ <strong>Improve generalization:</strong> Reduce overfitting to training data</li>
              <li>‚Ä¢ <strong>Enable deeper networks:</strong> Train models with more layers</li>
            </ul>
          </div>
        </section>
      </div>

      <!-- Chapter Navigation -->
      <ask-the-model-chapter-nav></ask-the-model-chapter-nav>
    </main>
  </div>

  <!-- Footer Component -->
  <ask-the-model-footer></ask-the-model-footer>

  <!-- Mobile Navigation Component -->
  <ask-the-model-mobile-nav></ask-the-model-mobile-nav>

  <!-- Web Components Script -->
  <script src="web-components.js"></script>
</body>
</html>
