<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Do LLMs Really Remember? Memory Explained - Ask The Model</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&family=Fira+Code&display=swap" rel="stylesheet">
  <script>
    tailwind.config = {
      theme: {
        extend: {
          fontFamily: {
            'inter': ['Inter', 'sans-serif'],
            'fira': ['Fira Code', 'monospace']
          }
        }
      }
    }
  </script>
</head>
<body class="bg-gray-900 text-gray-200 font-inter min-h-screen flex flex-col">
  <!-- Header Component -->
  <ask-the-model-header></ask-the-model-header>

  <div class="flex flex-col lg:flex-row flex-1">
    <!-- Sidebar Component -->
    <ask-the-model-sidebar></ask-the-model-sidebar>

    <!-- Main Content -->
    <main class="flex-1 p-4 md:p-8 lg:p-12 max-w-4xl mx-auto">
      <!-- Chapter Header -->
      <div class="mb-8">
        <h1 class="text-4xl md:text-5xl font-bold text-purple-400 mb-4">Do LLMs Really Remember? Memory Explained</h1>
        <p class="text-xl text-gray-300">Understanding how LLMs handle context and information</p>      <!-- Memory Concept -->
      <div class="mb-8 bg-gradient-to-r from-red-900/20 to-pink-900/20 p-6 rounded-lg border border-red-500/30">
        <h2 class="text-2xl font-semibold text-purple-400 mb-6 text-center">LLM Memory Mechanisms</h2>
        <div class="flex items-center justify-center space-x-6">
          <div class="text-center">
            <div class="w-16 h-16 bg-red-600 rounded-lg flex items-center justify-center mb-2">
              <span class="text-2xl">ðŸ§ </span>
            </div>
            <p class="text-sm text-red-300 font-semibold">Context Window</p>
            <p class="text-xs text-gray-300">Short-term</p>
          </div>
          <div class="text-purple-400 text-2xl">+</div>
          <div class="text-center">
            <div class="w-16 h-16 bg-blue-600 rounded-lg flex items-center justify-center mb-2">
              <span class="text-2xl">ðŸŽ¯</span>
            </div>
            <p class="text-sm text-blue-300 font-semibold">Attention</p>
            <p class="text-xs text-gray-300">Focus mechanism</p>
          </div>
          <div class="text-purple-400 text-2xl">+</div>
          <div class="text-center">
            <div class="w-16 h-16 bg-green-600 rounded-lg flex items-center justify-center mb-2">
              <span class="text-2xl">ðŸ“š</span>
            </div>
            <p class="text-sm text-green-300 font-semibold">External Tools</p>
            <p class="text-xs text-gray-300">RAG, Vector DBs</p>
          </div>
        </div>
      </div>


      </div>

      <!-- Chapter Content -->
      <div class="space-y-8">
        <section class="bg-gray-800 p-6 rounded-lg border border-gray-700">
          <h2 class="text-2xl font-semibold text-purple-400 mb-4">Context Windows and Forgetting</h2>
          <p class="text-gray-300 mb-4 leading-relaxed">
                    <strong>LLMs don't "remember" in the human sense</strong>. They have <strong>fixed context windows</strong> (e.g., 4K-128K tokens) that determine processing capacity. When conversations exceed this limit, <strong>older information gets truncated</strong>.
                </p>
                <div class="bg-gray-700 p-4 rounded-lg mt-4">
                    <h3 class="text-lg font-semibold text-purple-300 mb-2">Key Limitations:</h3>
                    <ul class="text-gray-300 text-sm space-y-1">
                        <li>â€¢ <strong>Fixed Memory:</strong> Cannot exceed context window size</li>
                        <li>â€¢ <strong>No Persistence:</strong> Each conversation starts fresh</li>
                        <li>â€¢ <strong>Information Loss:</strong> Early context gets forgotten</li>
                    </ul>
                </div>
        </section>
        <section class="bg-gray-800 p-6 rounded-lg border border-gray-700">
          <h2 class="text-2xl font-semibold text-purple-400 mb-4">Attention-Based Memory</h2>
          <p class="text-gray-300 mb-4 leading-relaxed">
                    <strong>Self-attention serves as working memory</strong>:
                </p>
                <div class="grid grid-cols-1 md:grid-cols-3 gap-4">
                    <div class="bg-gray-700 p-4 rounded-lg text-center">
                        <h3 class="text-lg font-semibold text-purple-300 mb-2">Attention Weights</h3>
                        <p class="text-gray-300 text-sm">Determine which previous tokens influence current predictions</p>
                    </div>
                    <div class="bg-gray-700 p-4 rounded-lg text-center">
                        <h3 class="text-lg font-semibold text-purple-300 mb-2">Key-Value Caches</h3>
                        <p class="text-gray-300 text-sm">Store recent context for efficient processing</p>
                    </div>
                    <div class="bg-gray-700 p-4 rounded-lg text-center">
                        <h3 class="text-lg font-semibold text-purple-300 mb-2">Positional Encodings</h3>
                        <p class="text-gray-300 text-sm">Help models track sequence order</p>
                    </div>
                </div>
        </section>
        <!-- Chapter Navigation Component -->
        <ask-the-model-chapter-nav></ask-the-model-chapter-nav>
      </div>
    </main>
  </div>
  
  <!-- Footer Component -->
  <ask-the-model-footer></ask-the-model-footer>

  <!-- Mobile Navigation Component -->
  <ask-the-model-mobile-nav></ask-the-model-mobile-nav>

  <!-- Web Components Script -->
  <script src="web-components.js"></script>
</body>
</html>